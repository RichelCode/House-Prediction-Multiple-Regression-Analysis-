---
title: "Stat 563 Project- Final"
author: "Richel Charntel Jing Jing"
date: "2024-11-19"
output: html_document
---



# Introduction

Understanding the factors that influence house prices is crucial for stakeholders in the real estate market, including buyers, sellers, and policymakers. This project leverages the **House Prices in King County, USA** dataset from Kaggle, which provides detailed information on house sales in King County, Washington. The dataset comprises 21,613 observations and 19 features, capturing a wide range of attributes, such as:

- **House characteristics**: Square footage, number of bedrooms, and bathrooms.
- **Location-based attributes**: Zip code and proximity to waterfronts.
- **Temporal factors**: Time of sale.

The primary goal of this project is to identify key predictors of house prices and develop a robust predictive model. By analyzing trends and patterns, we aim to uncover how various factors influence housing prices and their implications for the local real estate market. This includes examining systemic trends, such as the impact of location and property features on price variations.

To achieve this, we adopt a comprehensive methodology that includes:

- **Data preprocessing**: Ensuring data quality and consistency.

- **Exploratory data analysis (EDA)**: Identifying relationships and patterns.

- **Advanced modeling techniques**: Building predictive models to generate actionable insights.

This study seeks to deliver valuable insights and practical recommendations, empowering stakeholders to make informed decisions in the real estate market.


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
#Loading important Libraries
library(tidyverse)
library(ggplot2)
library(corrplot)
library(car)
library(tidyr)
library(dplyr)
library(lmtest)
library(MASS)
```

### Data Cleaning and Exploratory Data Analysis

We began by loading our dataset and inspecting the structure and summary statistics of the data. To facilitate the analysis, we installed and loaded the required libraries and packages. This included tools for data wrangling, visualization, and modeling. 

```{r 1.10, echo=FALSE, results='hide',message=FALSE, warning=FALSE}
#installing important packages
install.packages("corrplot")
```

```{r 1.11, echo=TRUE, results='hide', message=FALSE, warning=FALSE }
# Load the dataset
kc_house_data <- read.csv("C:\\Users\\EWURA\\Downloads\\Stat 563 Project\\kc_house_data.csv")

# View the first few rows
head(kc_house_data)
```

### Data Inspection

As a team, we started by exploring the structure and quality of the `kc_house_data` dataset:

- **Understanding the Dataset**: We reviewed its structure to identify variable types and gain a general sense of the dataset's composition.

- **Statistical Summary**: By examining basic descriptive statistics, we assessed patterns, identified potential outliers, and recognized unusual values.

- **Data Quality Check**:

  - **Missing Values**: We evaluated the dataset for missing entries to plan for data cleaning or imputation strategies.
  
  - **Duplicate Rows**: We checked for duplicate records, ensuring the dataset's integrity and avoiding redundancy in our analysis.


### Data Transformation

To prepare the dataset for analysis, we carried out key transformations:

- **Date Formatting**: The `date` column was reformatted to a standard date format, enabling time-based analysis and improving readability.

- **Categorical Variables**: Columns such as `waterfront`, `view`, `condition`, and `grade` were converted into categorical variables to ensure proper interpretation during statistical modeling.



```{r 1.12, echo=TRUE, results='hide', message=FALSE, warning=FALSE}
# Check structure and summary
str(kc_house_data)
summary(kc_house_data)

# Check for missing values
colSums(is.na(kc_house_data))

# Check for duplicate rows
nrow(kc_house_data) - nrow(distinct(kc_house_data))

```



```{r 1.13}
# Convert 'date' to Date type
kc_house_data$date <- as.Date(kc_house_data$date, format = "%Y%m%dT%H%M%S")

# Convert categorical-like columns to factors
kc_house_data$waterfront <- as.factor(kc_house_data$waterfront)
kc_house_data$view <- as.factor(kc_house_data$view)
kc_house_data$condition <- as.factor(kc_house_data$condition)
kc_house_data$grade <- as.factor(kc_house_data$grade)

```

### Filtering and Transformations

As a team, we applied several data preprocessing steps to clean and refine the dataset:

#### Boxplot and Initial Filtering
- **Boxplot for Price**:

  - We visualized the price distribution using a boxplot to detect outliers.
  
- **Filtering Extreme Values**:

  - We removed rows where the number of bedrooms exceeds 10, as such cases are rare and could skew the analysis.
  
  - Additionally, we excluded the top 1% of `sqft_lot` values to address extreme lot sizes that could distort statistical results while retaining 99% of the observations.
  

#### Extracting Temporal Features

- **Year and Month Sold**:

  - From the `date` column, we extracted the year (`year_sold`) and month (`month_sold`) of sale to facilitate time-based analysis.
  
- **Grouping by Decades**:

  - The `yr_built` variable was grouped into decades (`decade_built`) to observe trends over time.


### Outlier Recheck and Correlation Analysis

- **Rechecking Outliers**:

  - After filtering, we visualized the price distribution again. The boxplot appeared unchanged because extreme values of `sqft_lot` had little influence on price due to a weak positive correlation (0.112) between the two variables.
  
- **Correlation**:

  - We observed that the relationship between `sqft_lot` and `price` was weak, confirming that filtering based on `sqft_lot` did not significantly impact price outliers.


### Outlier Removal Using IQR

- **IQR-Based Filtering**:
  - We calculated the interquartile range (IQR) for the price variable and defined lower and upper bounds to identify potential outliers.
  
  - Rows with price values outside these bounds were removed from the dataset.


### Boxplot and Log Transformation

- **Post-Outlier Removal Boxplot**:

  - After applying the IQR method, we rechecked the boxplot for price.
  
- **Log Transformation**:

  - Given the heavy skewness of the price variable, we prepared to apply a log transformation. This step stabilizes variability and makes the distribution more symmetric, improving model performance.



```{r 1.14}
# Boxplot for price
boxplot(kc_house_data$price, main = "Boxplot for Price", horizontal = TRUE)

# Filter out extreme values
kc_house_data <- kc_house_data %>%
  filter(bedrooms <= 10, sqft_lot < quantile(sqft_lot, 0.99))

#Extracting year and month from the date column
#Grouping yr_built into decades
kc_house_data <- kc_house_data %>%
  mutate(year_sold = year(date),
         month_sold = month(date),
         decade_built = floor(yr_built / 10) * 10)
```

We filter out rows where the number of bedrooms exceeds 10; since this is an unusual case in real life. Not that we can have such a situation, but it is rare.
The 99th percentile here means we exclude only the top 1% of data points. These values are often outliers that represent unusually  rare lot sizes that can distort statistical analysis.
By keeping 99% of the data, we retain almost all the observations while removing extreme values that may heavily influence the results.


```{r 1.15}
# Recheck outliers after filtering
boxplot(kc_house_data$price, main = "Boxplot for Price (After Filtering)", horizontal = TRUE)

# Correlation between sqft_lot and price
cor <- cor(kc_house_data$sqft_lot, kc_house_data$price)
cor
```

The correlation between sqft_lot and price is 0.1120, thus a weak positive relationship. 
Given this weak positive correlation (0.1120) between sqft_lot and price, filtering based on sqft_lot does not significantly impact outliers in price. This is the reason why the boxplots for price displayed above remain unchanged even after filtering.


```{r 1.16}
# Calculate IQR for price
Q1 <- quantile(kc_house_data$price, 0.25)
Q3 <- quantile(kc_house_data$price, 0.75)
IQR <- Q3 - Q1

lower_bound <- Q1 - 1.5 * IQR
upper_bound <- Q3 + 1.5 * IQR

# Filter rows within bounds
kc_house_data <- kc_house_data %>%
  filter(price >= lower_bound & price <= upper_bound)

```



```{r 1.17}
# Recheck boxplot for price
boxplot(kc_house_data$price, main = "Boxplot for Price (After Removing Outliers)", horizontal = TRUE)

```

The IQR method is a common approach for dealing with outliers. It is not sufficient for this dataset due to the large range and skewness of price.
The price variable is heavily skewed,hence we apply a log transformation to stabilize the variability and make the distribution more symmetric.



### Exploratory Data Analysis

During our exploratory data analysis, we visualized and analyzed key variables in the dataset to understand their distributions, relationships, and potential outliers. The price distribution was right-skewed, with most properties priced below $500,000, indicating a concentration of mid-range housing. Scatterplots revealed positive correlations between price and variables such as living area and the number of bathrooms, although these relationships were not perfectly linear. Visualizations of categorical variables like grade and waterfront access highlighted their significant influence on price, with higher grades and waterfront properties commanding premium prices. 

Geospatial analysis showed localized clusters of higher-priced homes, underscoring the importance of location. A correlation heatmap further emphasized relationships between numerical variables, with notable multicollinearity observed among predictors like `sqft_living` and `sqft_above`. We also examined the effects of renovations, building age, and condition, finding that renovations and better conditions are associated with higher prices. Overall, these insights will guide our modeling efforts by highlighting important predictors and potential transformations for skewed variables.




```{r 1.18, echo=TRUE, results='hide', message=FALSE, warning=FALSE}
summary(kc_house_data$price)
summary(kc_house_data$sqft_living)
```


```{r 1.19}
ggplot(kc_house_data, aes(x = price)) +
  geom_histogram(bins = 30, fill = "blue", color = "white") +
  labs(title = "Distribution of House Prices", x = "Price", y = "Count")
```

The histogram above illustrates the distribution of house prices in King County. 
The distribution is right-skewed, with the majority of houses priced below $500,000. This suggests that most properties fall within a mid-range price category, while fewer properties are available at higher price points, with a long tail extending towards more expensive homes. 


```{r 1.20}
ggplot(kc_house_data, aes(x = sqft_living, y = price)) +
  geom_point(alpha = 0.5) +
  labs(title = "Price vs. Square Footage", x = "Square Footage", y = "Price")
```

The scatterplot shows the relationship between house prices and square footage in King County. There is a clear positive correlation, as larger houses (in terms of square footage) tend to have higher prices. However, the relationship is not perfectly linear, with some variability in price for houses with similar square footage, likely due to other influencing factors such as location, condition, or additional features. The plot also shows a few outliers where properties with extremely high square footage or price deviate significantly from the general trend.


```{r 1.21}
ggplot(kc_house_data, aes(x = as.factor(waterfront))) +
  geom_bar(fill = "purple") +
  labs(title = "Count of Waterfront Properties", x = "Waterfront (1 = Yes, 0 = No)", y = "Count")
```

The bar chart illustrates the distribution of waterfront properties in the dataset. It shows that the majority of houses in King County do not have waterfront access (represented by `0`), while a very small fraction of houses are located on waterfronts (represented by `1`).


```{r 1.22}
ggplot(kc_house_data, aes(x = sqft_living)) +
  geom_histogram(bins = 30, fill = "green", color = "white") +
  labs(title = "Distribution of Living Area", x = "Living Area (sqft)", y = "Count")
```

The histogram displays the distribution of living area (in square feet) for houses in King County. The data is slightly right-skewed, with the majority of homes having a living area between 1,000 and 3,000 square feet. A small number of houses have exceptionally large living areas, exceeding 4,000 square feet, which contribute to the skewness. This pattern highlights that most homes are of moderate size, while a few larger properties may significantly influence the mean living area.


```{r 1.23}
ggplot(kc_house_data, aes(x = as.factor(grade))) +
  geom_bar(fill = "orange") +
  labs(title = "Distribution of Grades", x = "Grade", y = "Count")
```

The bar chart shows the distribution of house grades in King County, where grades represent the overall construction and design quality. The majority of houses are assigned grades between 7 and 8, indicating average to slightly above-average quality. Very few houses fall at the extreme ends of the scale (grades 1–5 or 10–12), highlighting that most homes in this dataset are concentrated around the mid-range of the grading system.


```{r 1.24}
kc_house_data$floors <- ceiling(kc_house_data$floors)

kc_house_data$bathrooms <- ceiling(kc_house_data$bathrooms)

kc_house_data$bedrooms<- ceiling(kc_house_data$bedrooms)

# Compute correlation matrix
cor_matrix <- cor(kc_house_data %>% select_if(is.numeric), use = "complete.obs")

# Convert correlation matrix into a tidy format using tidyr
melted_cor <- as.data.frame(as.table(cor_matrix))

# Create the heatmap with viridis colors
ggplot(data = melted_cor, aes(x = Var1, y = Var2, fill = Freq)) +
  geom_tile(color = "white") +
  scale_fill_viridis_c(option = "plasma", name = "Correlation", limits = c(-1, 1)) +
  theme_minimal() +
  theme(
    axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1, size = 10),
    axis.text.y = element_text(size = 10),
    plot.title = element_text(size = 14, face = "bold"),
    legend.title = element_text(size = 10, face = "bold"),
    legend.text = element_text(size = 9)
  ) +
  labs(title = "Correlation Heatmap", x = "", y = "")
```

The correlation heatmap highlights the relationships among numerical variables in the dataset, revealing key patterns. House prices (price) show strong positive correlations with sqft_living and bathrooms, indicating that larger homes with more amenities are generally more expensive. Moderate correlations are observed between price and variables like grade and sqft_living15, suggesting that construction quality and neighborhood characteristics also influence prices. While variables like zipcode, yr_built, and yr_renovated show weak or negligible correlations with price, sqft_living and sqft_above exhibit high multicollinearity, indicating potential redundancy that must be managed in modeling. The heatmap effectively identifies important predictors and potential multicollinearity issues.


```{r 1.25}
ggplot(kc_house_data, aes(x = bathrooms, y = price)) +
  geom_point(alpha = 0.5) +
  labs(title = "Price vs. Bathrooms", x = "Number of Bathrooms", y = "Price")

```

The scatterplot shows the relationship between house prices and the number of bathrooms. There is a general trend indicating that houses with more bathrooms tend to have higher prices, as the points shift upwards with an increasing number of bathrooms. However, the data also exhibits significant clustering, especially at common values like 2 and 3 bathrooms, reflecting their prevalence in the dataset. The spread of prices widens as the number of bathrooms increases, suggesting greater variability in house features and values for homes with more bathrooms. Outliers are present, particularly for houses with an unusually high number of bathrooms, which may reflect luxury properties.

```{r 1.26}
ggplot(kc_house_data, aes(x = as.factor(waterfront), y = price, fill = as.factor(waterfront))) +
  geom_boxplot(outlier.color = "red", outlier.size = 2) +  # Customize outliers
  scale_fill_manual(values = c("skyblue", "orange")) +     # Add custom colors
  labs(
    title = "Price Distribution by Waterfront",
    x = "Waterfront (1 = Yes, 0 = No)",
    y = "Price",
    fill = "Waterfront"
  ) +
  theme_minimal() +
  theme(
    plot.title = element_text(size = 16, face = "bold"),
    axis.text = element_text(size = 12),
    axis.title = element_text(size = 14),
    legend.title = element_text(size = 12),
    legend.text = element_text(size = 10)
  )

```

The boxplot compares house prices between waterfront and non-waterfront properties. It clearly shows that waterfront properties (denoted by 1) tend to have significantly higher prices than non-waterfront properties (denoted by 0). The median price for waterfront properties is much higher, and their interquartile range (IQR) indicates greater variability in prices. Non-waterfront properties, while more common, have a lower median and a narrower IQR. This highlights the premium associated with waterfront properties due to their rarity and desirability.Outliers are present in both categories, indicating some exceptionally high-priced properties.



```{r 1.27}
ggplot(kc_house_data, aes(x = as.factor(grade), y = price, fill = as.factor(grade))) +
  geom_boxplot(outlier.color = "red", outlier.size = 2) +  # Customize outliers
  scale_fill_brewer(palette = "Set3") +                   # Use a predefined color palette
  labs(
    title = "Price Distribution by Grade",
    x = "Grade",
    y = "Price",
    fill = "Grade"
  ) +
  theme_minimal() +
  theme(
    plot.title = element_text(size = 16, face = "bold"),
    axis.text = element_text(size = 12),
    axis.title = element_text(size = 14),
    legend.title = element_text(size = 12),
    legend.text = element_text(size = 10)
  )

```

The boxplot shows the distribution of house prices across different grades, which represent the quality of construction and design. There is a clear upward trend, with higher grades associated with higher median prices and wider interquartile ranges (IQRs). Lower grades, such as 1 to 4, have narrow IQRs and lower prices, while grades 10 and above exhibit much higher medians and greater variability in price. The plot also reveals a higher frequency of outliers in the upper grades, indicating some exceptionally expensive properties. This strong relationship between grade and price highlights the importance of construction quality as a predictor for house prices.


```{r 1.28}
ggplot(kc_house_data, aes(x = sqft_living, y = price, color = as.factor(grade))) +
  geom_point(alpha = 0.6) +
  labs(title = "Price vs. Living Area by Grade", x = "Living Area (sqft)", y = "Price", color = "Grade")
```

The scatterplot illustrates the relationship between house prices and living area, with points color-coded by grade (quality of construction). There is a clear positive trend, where houses with larger living areas generally have higher prices. Additionally, higher-grade houses tend to cluster toward the upper end of both price and living area, indicating that grade amplifies the effect of living area on price. Lower-grade homes are concentrated in the lower price and living area range, while higher-grade homes span a broader range of living areas, reflecting their premium quality. This plot highlights the combined impact of living area and grade on house prices.


```{r 1.29}
ggplot(kc_house_data, aes(x = long, y = lat, color = price)) +
  geom_point(alpha = 0.5) +
  scale_color_viridis_c() +
  labs(title = "Geographic Distribution of House Prices", x = "Longitude", y = "Latitude", color = "Price")
```

The scatterplot maps the geographic distribution of house prices based on latitude and longitude. The color gradient represents house prices, with purple indicating lower prices and yellow indicating higher prices. Higher-priced homes are concentrated in specific areas, likely reflecting desirable neighborhoods or proximity to amenities such as waterfronts or urban centers. In contrast, lower-priced homes are more dispersed. This spatial pattern highlights the importance of location in determining house prices, demonstrating significant geographic variability within King County. The clustering of high-priced properties suggests localized demand for premium homes.



```{r 1.30}
ggplot(kc_house_data, aes(x = as.factor(floors), y = price, fill = as.factor(floors))) +
  geom_boxplot(outlier.color = "red", outlier.size = 2) +
  scale_fill_brewer(palette = "Pastel1") +
  labs(
    title = "Price Distribution by Number of Floors",
    x = "Number of Floors",
    y = "Price",
    fill = "Floors"
  ) +
  theme_minimal() +
  theme(
    plot.title = element_text(size = 16, face = "bold"),
    axis.text = element_text(size = 12),
    axis.title = element_text(size = 14),
    legend.title = element_text(size = 12),
    legend.text = element_text(size = 10)
  )

```

The boxplot illustrates the relationship between house prices and the number of floors. Houses with 1.5 to 2.5 floors exhibit higher median prices compared to single-story houses. As the number of floors increases, the range of prices also broadens, reflecting a greater variability in multi-floor properties. Outliers are evident for 1-floor and 3-floor homes, representing houses with significantly higher prices than others within the same category. This suggests that while the number of floors contributes to price, other factors might have a stronger influence.


```{r 1.31}
ggplot(kc_house_data, aes(x = yr_built, y = price, color = price)) +
  geom_point(alpha = 0.5) +
  scale_color_viridis_c() +
  labs(
    title = "Price vs. Year Built",
    x = "Year Built",
    y = "Price",
    color = "Price"
  ) +
  theme_minimal() +
  theme(
    plot.title = element_text(size = 16, face = "bold"),
    axis.text = element_text(size = 12),
    axis.title = element_text(size = 14)
  )

```

The scatterplot demonstrates the relationship between house prices and the year the house was built. The color gradient represents price levels, with yellow indicating higher-priced properties. Houses built in recent years (after 2000) show a tendency for higher prices compared to older homes. However, there is a considerable overlap in prices across years, suggesting that factors other than the year built, may play a more significant role in determining house prices.


```{r 1.32}
ggplot(kc_house_data, aes(x = as.factor(condition), y = price, fill = as.factor(condition))) +
  geom_boxplot(outlier.color = "red", outlier.size = 2) +
  scale_fill_brewer(palette = "Dark2") +
  labs(
    title = "Price Distribution by Condition",
    x = "Condition",
    y = "Price",
    fill = "Condition"
  ) +
  theme_minimal() +
  theme(
    plot.title = element_text(size = 16, face = "bold"),
    axis.text = element_text(size = 12),
    axis.title = element_text(size = 14),
    legend.title = element_text(size = 12),
    legend.text = element_text(size = 10)
  )

```

This boxplot highlights how house prices vary with the condition of the property, rated from 1 (poor) to 5 (excellent). As the condition improves, the median price increases, with homes in excellent condition (rating 5) having the highest median price. The variability of prices also grows with better condition ratings, reflecting the presence of premium houses in the higher ranges. Outliers are most prominent for lower condition ratings, indicating some houses in poor condition still command higher prices, likely due to other influential factors like location or unique features.


```{r 1.33}
kc_house_data %>%
  group_by(bedrooms, bathrooms) %>%
  summarise(count = n()) %>%
  ggplot(aes(x = as.integer(bedrooms), y = as.integer(bathrooms), fill = count)) +  # Convert to integers
  geom_tile(color = "white") +
  scale_fill_gradient(low = "lightblue", high = "darkblue", name = "Count") +
  scale_x_continuous(breaks = scales::breaks_pretty(n = 10)) +  # Ensure only integer labels
  scale_y_continuous(breaks = scales::breaks_pretty(n = 10)) +
  labs(title = "Bathrooms vs. Bedrooms Heatmap", x = "Number of Bedrooms", y = "Number of Bathrooms") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

```

The heatmap illustrates the distribution of houses based on the number of bedrooms and bathrooms, with darker shades indicating higher counts. The most common configurations are houses with 3 to 4 bedrooms and 2 to 3 bathrooms, as reflected by the darkest blue region. Houses with fewer or more bedrooms and bathrooms are less frequent, as indicated by the lighter shades in other areas of the heatmap.This trend suggests that the dataset is dominated by mid-sized homes, which likely cater to average family sizes.

```{r 1.34}
# Renovated vs. Non-Renovated Boxplot
kc_house_data %>%
  mutate(renovated = ifelse(yr_renovated > 0, "Yes", "No")) %>%
  ggplot(aes(x = renovated, y = price, fill = renovated)) +
  geom_boxplot(outlier.color = "red", outlier.size = 2) +
  scale_fill_manual(values = c("lightgreen", "lightcoral")) +
  labs(title = "Price Distribution by Renovation Status", x = "Renovated", y = "Price", fill = "Renovated") +
  theme_minimal()

```

The boxplot compares house prices for renovated and non-renovated properties. Renovated houses exhibit a higher median price and a slightly larger interquartile range compared to non-renovated properties, reflecting their added value. Additionally, there is less variability in prices for renovated homes, suggesting that they tend to be clustered in specific price ranges. Outliers are more prominent in non-renovated properties, indicating that some of these houses command significantly higher prices, likely due to other factors.


```{r 1.35}
# Year Built Histogram
ggplot(kc_house_data, aes(x = yr_built)) +
  geom_histogram(binwidth = 5, fill = "dodgerblue", color = "white") +
  labs(title = "Distribution of Year Built", x = "Year Built", y = "Count") +
  theme_minimal()
```

The histogram displays the distribution of houses by the year they were built. There is a noticeable increase in the number of houses constructed from the mid-20th century, peaking around the early 2000s, likely reflecting housing market booms. Construction activity is lower for homes built before 1925 and after 2010, suggesting that newer constructions are relatively rare in the dataset. This distribution provides insights into the age of the housing stock in the dataset, with most houses built between 1950 and 2000.


### Model Building and Diagnostics

For the model-building phase, we employed multiple linear regression to predict house prices using a comprehensive set of predictors. Initial diagnostics indicated multicollinearity issues, particularly between `yr_built` and `decade_built`, which were addressed by removing the latter. The refined model achieved an \( R^2 \) of 71.25%, demonstrating that approximately 71% of the variability in the log-transformed house prices could be explained by the predictors. Significant variables included `sqft_living`, `bathrooms`, `grade`, `lat`, and `waterfront`, highlighting the impact of structural features, location, and quality on pricing. 

Advanced methods, including forward, backward, and stepwise selection, were used to optimize model performance, with backward selection yielding the best fit. Interaction terms and diagnostics (Residuals vs. Fitted, Q-Q plots, and Scale-Location plots) identified non-linearity and heteroscedasticity, prompting further refinements. Ultimately, a refined model was selected that balanced interpretability and predictive power while minimizing multicollinearity and residual errors. 

````{r 1.36, echo=TRUE, results='hide', message=FALSE, warning=FALSE}
# Remove 'id' and 'date'
kc_house_data <- kc_house_data[, !names(kc_house_data) %in% c("id", "date")]

# Fit the full model
full_model <- lm(price ~ ., data = kc_house_data)

# Fit the full regression model
full_model <- lm(price ~ ., data = kc_house_data)

# Summarize the model
summary(full_model)

````

The multiple linear regression model explains 70.36% of the variability in house prices (\(R^2 = 0.7036\)) and 70.31% when adjusted for the number of predictors (\( \text{Adjusted } R^2 = 0.7031\)). The residual standard error is 113,200, indicating the average deviation of predicted prices from observed prices. Significant predictors (\(p < 0.05\)) include structural features such as bedrooms, bathrooms,sqft_living, and floors, as well as location variables like waterfront, view, zipcode, lat, and long. Temporal factors like year_sold and month_sold are also significant, reflecting the impact of market timing on house prices. Some levels of categorical variables (grade10, grade11, grade12) and conditions are also influential, whereas lower grades (grade3 to grade8) and certain conditions are not statistically significant (\(p > 0.05\)). Latitude (lat) and square footage (sqft_living) have strong positive effects, with each additional square foot increasing price by \$73.82 on average. 


```{r 1.37, echo=TRUE, results='hide', message=FALSE, warning=FALSE}
# Remove aliased columns (sqft_above and sqft_basement)
kc_house_data <- kc_house_data[, !names(kc_house_data) %in% c("sqft_above", "sqft_basement")]

# Refit the full model
full_model <- lm(price ~ ., data = kc_house_data)

# Summarize the updated model
summary(full_model)

# Check for multicollinearity
vif(full_model)

```

The Variance Inflation Factor (VIF) analysis reveals minimal multicollinearity among most predictors, with VIF values generally well below the critical threshold of 10. However, yr_built and decade_built have exceptionally high VIF values (105.81 and 104.38, respectively), indicating severe multicollinearity, likely due to redundancy between these variables. Other variables, such as sqft_living (VIF = 4.42) and bathrooms (VIF = 2.99), exhibit moderate multicollinearity, which is manageable. Predictors like waterfront, zipcode, and lat have low VIF values (all under 2), suggesting independence. While the overall multicollinearity is not a significant concern for most predictors, addressing the collinearity between yr_built and decade_built is crucial.


```{r 1.38, echo=TRUE, results='hide', message=FALSE, warning=FALSE}
# Remove 'decade_built' using base R
kc_house_data <- kc_house_data[, !names(kc_house_data) %in% "decade_built"]

# Fit the updated full model
full_model <- lm(price ~ ., data = kc_house_data)

# Check VIF again
vif(full_model)

```

After addressing multicollinearity by removing decade_built, the VIF values for all predictors have significantly improved, with no values exceeding the critical threshold of 10. The variable yr_built, which previously exhibited severe multicollinearity, now has a VIF of 2.55, indicating low multicollinearity and making it suitable for inclusion in the model. Most predictors, such as bathrooms, sqft_living, sqft_living15, and zipcode, show moderate multicollinearity with VIF values between 2 and 4, which is manageable. Other predictors, including waterfront, lat, and long, have low VIF values close to 1, indicating high independence from other variables. The removal of decade_built successfully resolved the multicollinearity issue, and the model is now more stable and interpretable. Further analysis can proceed without concerns of redundancy among predictors.


```{r 1.39}
plot(full_model$fitted.values, residuals(full_model), 
     main = "Residuals vs Fitted", 
     xlab = "Fitted Values", 
     ylab = "Residuals")
abline(h = 0, col = "red")
```

The "Residuals vs Fitted" plot is used to check the  assumption of linearity in a regression model, which states that the relationship between the predictors and the response variable is linear. If the model satisfies the linearity assumption, the residuals should appear randomly scattered around the horizontal line at zero, without any discernible patterns. In this plot, while the residuals are generally centered around zero, there are some deviations, particularly at extreme fitted values, which may suggest potential non-linear relationships between the predictors and the response variable. This indicates that the linearity assumption might not be fully satisfied.

```{r 1.40}
qqnorm(residuals(full_model), main = "Normal Q-Q Plot")
qqline(residuals(full_model), col = "red")
```

The "Normal Q-Q Plot" is used to assess whether the residuals of the model follow a normal distribution, which is an important assumption in linear regression. In this plot, the residual quantiles are plotted against theoretical quantiles of a normal distribution. If the residuals are normally distributed, the points should align closely along the red diagonal line. In this plot, the majority of the points fall near the line, suggesting approximate normality. However, deviations at both ends (tails) indicate potential outliers or skewness in the residuals. This observation suggests that the normality assumption may not be fully satisfied, and a transformation of the response variable will be considered.


```{r 1.41}
plot(full_model, which = 3)  # Scale-Location plot
```

The Scale-Location plot, or Spread-Location plot, is used to assess the assumption of constant variance. In this plot, the square root of the standardized residuals is plotted against the fitted values, and ideally, the points should be randomly scattered around a horizontal line without any discernible pattern. However, in the provided plot, the red line shows a slight upward curvature as the fitted values increase, suggesting potential heteroscedasticity, where the residual variance is not constant. This indicates a violation of the constant variance assumption in the model. To address this issue, potential remedies include applying a transformation to the response variable to stabilize variance.





#Checking the constant variance assumption

Hypothesis: H_0:Error terms have constant variance
            H_a: Error terms do not have constant variance 
            
```{r 1.42}
bptest(full_model)
```



For the provided model, the test statistic (BP) is 1757.3, with 33 degrees of freedom and a p-value < 2.2e-16. The extremely small p-value indicates strong evidence against the null hypothesis, suggesting that heteroscedasticity is present in the model. This result confirms that the variance of residuals is not constant, consistent with observations from the Scale-Location plot. 



#Testing the independence assumption

Hypothesis: H_0:Error terms are uncorrelated
            H_a: Error terms are correlated
            
```{r 1.43}
dwtest(full_model)
```

            
The Durbin-Watson statistic (DW) is 1.9745, which is close to 2, suggesting minimal autocorrelation in the residuals. However, the p-value (0.03432) is lower than 0.05, indicating evidence of positive autocorrelation and rejecting the null hypothesis of no autocorrelation. This result suggests the residuals may not be completely independent, warranting further investigation or model adjustments.



```{r 1.44}
plot(full_model, which = 5)  # Residuals vs Leverage
```

The Residuals vs. Leverage plot is used to identify influential points in a regression model that may have a significant impact on the model's fit. In this graph, the standardized residuals are plotted against leverage, with Cook's distance contours overlaid. Points with high leverage and large residuals can be highly influential, as they may disproportionately affect the regression coefficients.


```{r 1.45}
# Perform Box-Cox transformation
boxcox_result <- boxcox(full_model, lambda = seq(-2, 2, by = 0.1))
```

The Box-Cox transformation plot indicates that the optimal $\lambda$ is approximately $0$, aligning with the use of a logarithmic transformation for the response variable (price).


```{r 1.46, echo=TRUE, results='hide', message=FALSE, warning=FALSE}
kc_house_data$log_price <- log(kc_house_data$price)
full_model_log <- lm(log_price ~ ., data = kc_house_data)
full_model_log
```

The linear model with the log-transformed response variable (log_price) adjusts relationships between predictors and the outcome, with coefficients now representing percentage changes in price. Key predictors like sqft_living (5.726e-06) and bathrooms (1.338e-02) show positive associations with higher log-transformed prices, while waterfront1 (1.064e-01), property condition, and grade levels also have strong positive impacts. Variables such as yr_built (-1.847e-04) exhibit slight negative effects. 


```{r 1.47}
# Residuals vs Fitted
plot(full_model_log, which = 1)
```

The Residuals vs Fitted plot shows a curved trend, suggesting potential non-linearity between predictors and the response variable. The varying spread of residuals indicates heteroscedasticity, where variance is not constant across fitted values. Additionally, observation 11798 appears to be an outlier, which may affect model performance. 

```{r 1.48}
# Normal Q-Q Plot
plot(full_model_log, which = 2)

```

The Q-Q plot evaluates whether the residuals follow a normal distribution: The residuals mostly align with the diagonal line, indicating approximate normality in the central range.  Deviations at the tails (both left and right) suggest potential non-normality in extreme residuals. These issues may affect inference, and addressing outliers or transforming the response variable could help.

```{r 1.49}
# Scale-Location Plot
plot(full_model_log, which = 3)
```

The Scale-Location plot checks for homoscedasticity (constant variance) in residuals：The red line shows a curved pattern, suggesting potential heteroscedasticity. Residual spread increases with fitted values, indicating non-constant variance. Consider transformations to address heteroscedasticity.

```{r 1.50}
# Residuals vs Leverage Plot
plot(full_model_log, which = 5)

```

The diagnostic plots for the log-transformed model indicate potential model assumption violations. The Residuals vs. Fitted plot shows a noticeable curve, suggesting non-linearity in the relationship between predictors and the log-transformed response variable. The Normal Q-Q plot reveals deviations at the tails, indicating potential issues with normality of residuals. The Scale-Location plot exhibits a fan-shaped pattern, suggesting heteroscedasticity where variance increases with fitted values. Finally, the Residuals vs. Leverage plot highlights influential data points indicating that certain observations exert high leverage on the model and may require further investigation.




#Testing for Homoscedasticity

#Hypothesis:
H_0: Homoscedasticity (constant variance of residuals).
H_a: heteroscedasticity (variance of residuals depends on predictors).

```{r 1.51}
# Breusch-Pagan Test for Homoscedasticity
bptest(full_model_log)
```


The Breusch-Pagan test evaluates the presence of heteroscedasticity in the residuals. With a BP statistic of 2354.8 and a p-value < 2.2e-16, the null hypothesis of homoscedasticity is rejected, indicating significant heteroscedasticity. This suggests that the variance of residuals changes across the range of fitted values



#Testing the independence assumption

Hypothesis: H_0:Error terms are uncorrelated
            H_a: Error terms are correlated
            
```{r 1.52}
# Durbin-Watson Test for Autocorrelation
dwtest(full_model_log)

```


The Durbin-Watson test assesses autocorrelation in the residuals. Here, the DW statistic of 2.0134 and a high p-value of 0.8286 indicate that the null hypothesis of no autocorrelation cannot be rejected. This result suggests that there is no significant autocorrelation in the residuals, and no further action is required to address serial correlation in this model.



# Checking multicollinearity among predictors
````{r 1.53, echo=TRUE, results='hide', message=FALSE, warning=FALSE}
# Variance Inflation Factor (VIF) for Multicollinearity
vif(full_model_log)
````

The VIF checks for multicollinearity among predictors. All VIF values are below 10, indicating no severe multicollinearity in the model, as higher values (>10) typically signal problematic correlation.



````{r 1.54, echo=TRUE, results='hide', message=FALSE, warning=FALSE}
#Removing influential points from the data
cooks <- cooks.distance(full_model_log)  # Calculate Cook's Distance
threshold <- 4 / nrow(kc_house_data)     # Define threshold
influential_points <- which(cooks > threshold)  # Get indices of influential points

# Exclude influential points from the data
refined_data <- kc_house_data[-influential_points, ]
````


````{r 1.55, echo=TRUE, results='hide', message=FALSE, warning=FALSE}
# Refit the model
full_model_log_cleaned <- lm(log_price ~ ., data = refined_data)

# Summary of the new model
summary(full_model_log_cleaned)

````

The refined regression model, after removing influential points, exhibits a strong fit to the data with a residual standard error of 0.07233 and an \( R^2 \) value of 0.9687, indicating that approximately 97% of the variation in the log-transformed price is explained by the predictors. Key variables such as `price`, `bathrooms`, `sqft_living`, `floors`, `waterfront1`, and `latitude` are highly significant (\( p < 0.001 \)), contributing substantially to the model. However, some predictors, like `sqft_lot15` and `month_sold`, are not statistically significant, suggesting they could be considered for exclusion in further refinement. The overall \( F \)-statistic (\( 1.834 \times 10^4 \), \( p < 0.001 \)) confirms the model's predictive power. 


```{r 1.56}
# Diagnostic Plots for the Refined Model
# Generate diagnostic plots
par(mfrow = c(2, 2)) # Set up for 2x2 grid of plots
plot(full_model_log_cleaned) # Automatically generates Residuals vs Fitted, Q-Q, Scale-Location, and Residuals vs Leverage plots
par(mfrow = c(1, 1)) # Reset plot layout
```

Residuals vs Fitted：This plot checks for non-linearity and heteroscedasticity. The curved red line suggests some non-linearity, and the spread of residuals indicates potential heteroscedasticity.
Q-Q Plot：The Q-Q plot assesses normality of residuals. Most points align with the diagonal line, but deviations at the tails suggest that extreme residuals may not follow a normal distribution.
Scale-Location：This plot examines homoscedasticity (constant variance). The red line's curvature and the increasing spread of residuals indicate heteroscedasticity, as variance is not constant across fitted values.
Residuals vs Leverage：This plot identifies influential points. Observations with high leverage (e.g., 12510 and 1448) may disproportionately affect the model and should be investigated further.




#Testing for Homoscedasticity

#Hypothesis:
H_0: Homoscedasticity (constant variance of residuals).
H_a: heteroscedasticity (variance of residuals depends on predictors)

```{r 1.57}
bptest(full_model_log_cleaned)
```
The Breusch-Pagan test evaluates heteroscedasticity in the residuals. With a BP statistic of 2030.2 (df = 32) and a p-value < 2.2e-16, the null hypothesis of homoscedasticity is rejected, indicating significant heteroscedasticity even after cleaning the data. This suggests that the variance of residuals is not constant.

```{r 1.58, echo=TRUE, results='hide', message=FALSE, warning=FALSE}
forward_model <- step(lm(log_price ~ 1, data = refined_data), 
                      scope = ~ sqft_living + bathrooms + floors + bedrooms + grade + view + waterfront + sqft_lot + condition + yr_built + yr_renovated + lat + long + sqft_living15 + sqft_lot15 + year_sold + month_sold + zipcode, direction = "forward")
```

```{r 1.59, echo=TRUE, results='hide', message=FALSE, warning=FALSE}
backward_model <- step(lm(log_price ~ ., data = refined_data), 
                       direction = "backward")
```


```{r 1.60, echo=TRUE, results='hide', message=FALSE, warning=FALSE}
both_model <- step(lm(log_price ~ 1, data = refined_data), 
                   scope = ~ sqft_living + bathrooms + floors + bedrooms + grade + view + waterfront + sqft_lot + condition + yr_built + yr_renovated + lat + long + sqft_living15 + sqft_lot15 + year_sold + month_sold + zipcode , 
                   direction = "both")

```


```{r 1.61, echo=TRUE, results='hide', message=FALSE, warning=FALSE}
# Function to compute metrics for each model
compute_model_metrics <- function(model, data) {
  sse <- sum(residuals(model)^2)
  r2 <- summary(model)$r.squared
  adj_r2 <- summary(model)$adj.r.squared
  aic <- AIC(model)
  bic <- BIC(model)
  
  # Calculate Cp (Mallow's Cp)
  cp <- sse / (var(model$residuals) * (nrow(data) - length(coef(model)))) + 2 * length(coef(model)) - nrow(data)
  
  # Calculate PRESS
  press <- sum((residuals(model) / (1 - lm.influence(model)$hat))^2)
  
  return(data.frame(SSE = sse, R2 = r2, Adj_R2 = adj_r2, AIC = aic, BIC = bic, Cp = cp))
}

# Compute metrics for all models
metrics_forward <- compute_model_metrics(forward_model, refined_data)
metrics_backward <- compute_model_metrics(backward_model, refined_data)
metrics_both <- compute_model_metrics(both_model, refined_data)

# Combine the results
results <- rbind(
  cbind(Model = "Forward", metrics_forward),
  cbind(Model = "Backward", metrics_backward),
  cbind(Model = "Both", metrics_both)
)

# Display the table
results

```

### Model Comparison and Selection

We compared three models using forward selection, backward elimination, and stepwise (both) selection techniques. The performance metrics for these models are summarized in the table below:

| Model     | SSE       | R²         | Adjusted R² | AIC       | BIC       | Cp      |
|-----------|-----------|------------|-------------|-----------|-----------|---------|
| Forward   | 904.55562 | 0.7145100  | 0.7140734   | -3861.354 | -3617.946 | -18933  |
| Backward  | 99.24911  | 0.9686756  | 0.9686244   | -45830.501| -45571.389| -18929  |
| Both      | 904.55562 | 0.7145100  | 0.7140734   | -3861.354 | -3617.946 | -18933  |



The **backward elimination model** was selected based on the following performance metrics:
- **Lowest SSE (99.24911)**: Indicates the smallest residual sum of squares, representing better fit.
- **Highest \( R^2 \) (0.9686756) and Adjusted \( R^2 \) (0.9686244)**: Explains the most variability in \( \log(\text{price}) \) while adjusting for the number of predictors.
- **Lowest AIC (-45830.501) and BIC (-45571.389)**: Demonstrates model efficiency with fewer predictors.

In contrast, the **Forward Selection** and **Both** models yield identical results, with higher SSE (904.82), lower R² (0.7145), and Adjusted R² (0.7141), along with less favorable AIC (-3856.99) and BIC (-3613.58), making them less effective compared to Backward Selection. Thus, Backward Selection demonstrates the best balance between model fit and complexity among the three methods.


### Model Diagnostics for the Backward Model

```{r 1.62}
 # Residuals vs Fitted
plot(resid(backward_model) ~ fitted(backward_model), 
     main = "Residuals vs Fitted", 
     xlab = "Fitted Values", 
     ylab = "Residuals")
abline(h = 0, col = "red")

```

The Residuals vs Fitted plot shows a clear curved pattern, indicating non-linearity between predictors and the response variable. 

```{r 1.63}
# Q-Q Plot
qqnorm(resid(backward_model), main = "Normal Q-Q Plot")
qqline(resid(backward_model), col = "red")

```
 
The Normal Q-Q Plot assesses residual normality. Most points align well with the red line, indicating approximate normality, but deviations at the tails suggest potential outliers or non-normality in extreme values.

```{r 1.64}

# Scale-Location Plot
plot(sqrt(abs(resid(backward_model))) ~ fitted(backward_model), 
     main = "Scale-Location", 
     xlab = "Fitted Values", 
     ylab = "Sqrt(|Residuals|)")
abline(h = 0, col = "red")

```

The Scale-Location plot shows the square root of residuals against fitted values to assess constant variance. The fan-shaped pattern indicates that residual variance increases with fitted values, suggesting non-constant variance. 
```{r 1.65}
# Residuals vs Leverage
plot(backward_model, which = 5) # Cook's Distance
```

After the diagnostics on our backward model, the predictors zipcode, yr_renovated, sqft_lot15, and month_sold were removed from the model as they showed minimal contributions to reducing the residual sum of squares (RSS) and had negligible impact on improving the model's overall fit, as indicated by their low sum of squares in the backward elimination output. Additionally, these predictors had higher p-values, suggesting they were not statistically significant in explaining the variability in the response variable. 

### Considering Refined Versions of the Backward Selection Model

```{r 1.66, echo=TRUE, results='hide', message=FALSE, warning=FALSE}
# Updating the backward model thus removing insignificant predictors
model_refined <- lm(log_price ~ grade + lat + sqft_living + yr_built +bedrooms+
                      sqft_living15 + floors + condition + view +
                      year_sold + bathrooms + waterfront, data = refined_data)
summary(model_refined)

```


#Interpretation: 

The R-squared is 0.7125 indicating that 71.25% of the variability in the response variable (log_price) is explained by the predictors in the model. This is a strong result for a real-world model.
The F-statistic (1880 on 25 and 18969 DF,  p-value: < 2.2e-16) confirms that the overall model is statistically significant.
The residual standard error (0.2192) is small indicating a good fit.
The model balances the number of predictors and explanatory power well, as shown by the close R-squared(0.7125) and adjusted R-squared (0.7121).


```{r 1.67}
# Function to compute metrics for each model
compute_model_metrics <- function(model, data) {
  sse <- sum(residuals(model)^2)
  r2 <- summary(model)$r.squared
  adj_r2 <- summary(model)$adj.r.squared
  aic <- AIC(model)
  bic <- BIC(model)
  
  # Calculate Cp (Mallow's Cp)
  cp <- sse / (var(model$residuals) * (nrow(data) - length(coef(model)))) + 2 * length(coef(model)) - nrow(data)
  
  
  return(data.frame(SSE = sse, R2 = r2, Adj_R2 = adj_r2, AIC = aic, BIC = bic, Cp = cp))
}

# Compute metrics for all models
metrics_refined <- compute_model_metrics(model_refined, refined_data)

# Combine the results
results <- rbind(
  cbind(Model = "Refined", metrics_refined)
)

# Display the table
results

```
### Model Diagnostics for the Refined Backward Selection Model

```{r 1.68}

# Plot diagnostic plots for model_refined
par(mfrow = c(2, 2))  # Set up a 2x2 grid for plots
plot(model_refined)

```

Residuals vs Fitted：This plot checks for non-linearity and constant variance. The residuals appear evenly spread around the red line, indicating no major issues with linearity or variance.
Q-Q Plot ：This plot assesses normality of residuals. Most points align with the diagonal line, but deviations at the tails suggest some outliers or slight non-normality.
Scale-Location  ：This plot examines the spread of residuals for constant variance. The relatively flat red line indicates that residual variance is fairly constant across fitted values.
Residuals vs Leverage  ：This plot identifies influential points. Observations like 14767 have high leverage and should be examined to assess their impact on the model.


### Considering Interaction Terms 


```{r 1.69}
# Fit models with interaction terms
model_1 <- lm(log_price ~ . + waterfront:condition, data = refined_data)
model_2 <- lm(log_price ~ . + waterfront:grade, data = refined_data)
model_3 <- lm(log_price ~ . + bathrooms:floors, data = refined_data)
model_4 <- lm(log_price ~ . + sqft_living:condition, data = refined_data)
model_5 <- lm(log_price~  . + condition:grade, data = refined_data)

# Compute metrics for each model
metrics_1 <- compute_model_metrics(model_1, refined_data)
metrics_2 <- compute_model_metrics(model_2, refined_data)
metrics_3 <- compute_model_metrics(model_3, refined_data)
metrics_4 <- compute_model_metrics(model_4, refined_data)
metrics_5 <- compute_model_metrics(model_5, refined_data)


# Combine metrics into a table
interaction_results <- rbind(
  cbind(Model = "Waterfront:Condition", metrics_1),
  cbind(Model = "Waterfront:Grade", metrics_2),
  cbind(Model = "Bathrooms:Floors", metrics_3),
  cbind(Model = "Sqft Living:Condition", metrics_4),
  cbind(Model = "Condition:Grade", metrics_5)
 
)

# Display the table
interaction_results

```

### Interaction Term Analysis

We evaluated multiple interaction terms to identify the best-performing model based on key metrics such as \( R^2 \), Adjusted \( R^2 \), SSE, AIC, BIC, and Cp. The table below summarizes the results:

| Interaction Term        | SSE       | R²         | Adjusted R² | AIC       | BIC       | Cp      |
|--------------------------|-----------|------------|-------------|-----------|-----------|---------|
| Waterfront:Condition     | 99.24308  | 0.9686775  | 0.9686230   | -45827.66 | -45552.84 | -18919  |
| Waterfront:Grade         | 99.24035  | 0.9686784  | 0.9686206   | -45824.18 | -45533.66 | -18911  |
| Bathrooms:Floors         | 98.95638  | 0.9687680  | 0.9687137   | -45882.61 | -45607.79 | -18925  |
| Sqft Living:Condition    | 99.05219  | 0.9687378  | 0.9686784   | -45858.23 | -45559.85 | -18919  |
| Condition:Grade          | 97.97284  | 0.9690784  | 0.9689985   | -46040.33 | -45639.89 | -18863  |

The **Condition:Grade** interaction term emerges as the best-performing model. It has:
- The lowest SSE (97.97284), indicating the smallest residual sum of squares.
- The highest \( R^2 \) (0.9690784) and Adjusted \( R^2 \) (0.9689985), showing strong explanatory power.
- The lowest AIC (-46040.33) and BIC (-45639.89), demonstrating model efficiency.
- A relatively low Cp value (-18863), indicating that the model strikes a good balance between bias and variance.

Although other interactions, such as **Bathrooms:Floors**, performed well, the superior performance of the **Condition:Grade** model across all metrics makes it the optimal choice for further analysis.


### Diagnostics for Condition:Grade Interation Model
```{r 1.70}
# Graphical diagnostics
par(mfrow = c(2, 2))  # Layout for 4 plots
plot(model_5)  # Produces Residuals vs Fitted, Q-Q Plot, Scale-Location, and Residuals vs Leverage
```




## Diagnostic Plots for Model Assumptions

The diagnostic plots above provide insights into the assumptions of the linear regression model. The **Residuals vs Fitted** plot reveals a noticeable curvature, suggesting potential non-linearity between predictors and the response variable. The **Q-Q Plot** indicates approximate normality for the residuals in the central range, but deviations at the tails suggest the presence of outliers or non-normality in extreme values. The **Scale-Location Plot** exhibits a fan-shaped pattern, indicating heteroscedasticity where the variance of residuals increases with fitted values. Lastly, the **Residuals vs Leverage** plot identifies influential points that exert a significant impact on the model, as indicated by Cook's distance. These findings suggest that model refinements, such as transformations or addressing influential points, are necessary to improve the model's validity.

### Considering other Models and Evaluating Performance

We further considered the refined model by testing the exclusion of specific variables (`grade` and `condition`) to assess their impact on model performance. Three models are fitted: one without `grade` (`model_refined1`), one without `condition` (`model_refined2`), and one without both `grade` and `condition` (`model_refined3`). The `compute_model_metrics` function is then applied to calculate key performance metrics for each model, including 
SSE, \( R^2 \), Adjusted \( R^2 \), AIC, BIC, and Cp, which help evaluate and compare model efficiency and fit. The results are summarized in a table for easy comparison, highlighting the model that best balances explanatory power. Finally, diagnostic plots are generated for the best model among the three to examine its assumptions, such as linearity, normality, and homoscedasticity.

```{r 1.71, echo=TRUE, results='hide', message=FALSE, warning=FALSE}
# Refined model without grade
model_refined1 <- lm(log_price ~lat + sqft_living + yr_built +bedrooms+
                      sqft_living15 + floors + condition + view +
                      year_sold + bathrooms + waterfront, data = refined_data)
summary(model_refined1)

```


```{r 1.72, echo=TRUE, results='hide', message=FALSE, warning=FALSE}
# Refined model without condition
model_refined2 <- lm(log_price ~lat + sqft_living + yr_built +bedrooms+
                      sqft_living15 + floors + grade + view +
                      year_sold + bathrooms + waterfront, data = refined_data)
summary(model_refined2)

```


```{r 1.73, echo=TRUE, results='hide', message=FALSE, warning=FALSE}
# Refined model without condition and grade
model_refined3 <- lm(log_price ~lat + sqft_living + yr_built +bedrooms+
                      sqft_living15 + floors + view +
                      year_sold + bathrooms + waterfront, data = refined_data)
summary(model_refined3)

```


```{r 1.74}
# Function to compute metrics for each model
compute_model_metrics <- function(model, data) {
  sse <- sum(residuals(model)^2)
  r2 <- summary(model)$r.squared
  adj_r2 <- summary(model)$adj.r.squared
  aic <- AIC(model)
  bic <- BIC(model)
  
  # Calculate Cp (Mallow's Cp)
  cp <- sse / (var(model$residuals) * (nrow(data) - length(coef(model)))) + 2 * length(coef(model)) - nrow(data)
  
  # Calculate PRESS
  press <- sum((residuals(model) / (1 - lm.influence(model)$hat))^2)
  
  return(data.frame(SSE = sse, R2 = r2, Adj_R2 = adj_r2, AIC = aic, BIC = bic, Cp = cp))
}

# Compute metrics for all models
metrics_forward <- compute_model_metrics(model_refined1, refined_data)
metrics_backward <- compute_model_metrics(model_refined2, refined_data)
metrics_both <- compute_model_metrics(model_refined3, refined_data)

# Combine the results
results <- rbind(
  cbind(Model = "Without Grade", metrics_forward),
  cbind(Model = "Without Condition", metrics_backward),
  cbind(Model = "Without Grade and Condition", metrics_both)
)

# Display the table
results

```

### Model Comparison: Refining Predictors

This analysis compares three refined models, each excluding certain predictors (`grade` and/or `condition`), to evaluate their performance across key metrics such as SSE, \( R^2 \), Adjusted \( R^2 \), AIC, BIC, and Cp. The results are summarized in the table below:

| Model                   | SSE       | R²         | Adjusted R² | AIC        | BIC        | Cp      |
|--------------------------|-----------|------------|-------------|------------|------------|---------|
| Without Grade            | 1068.2875 | 0.6628340  | 0.6625319   | -725.3540  | -576.1683  | -18957  |
| Without Condition        | 928.0638  | 0.7070905  | 0.7067662   | -3390.0313 | -3209.4381 | -18949  |
| Without Grade and Condition | 1085.9223 | 0.6572682  | 0.6570334   | -422.3686  | -304.5905  | -18965  |

The model **"Without Condition"** outperforms the others, with:
- The lowest SSE (928.0638), indicating minimal residual errors.
- The highest \( R^2 \) (0.7071) and Adjusted \( R^2 \) (0.7068), reflecting strong explanatory power.
- The most favorable AIC (-3390.0313) and BIC (-3209.4381), demonstrating the model's efficiency.

While the models "Without Grade" and "Without Grade and Condition" show lower SSE, \( R^2 \), and Adjusted \( R^2 \), their higher AIC and BIC values suggest they are less efficient. Thus, the model excluding `condition` provides the best balance of accuracy and predictive power and is selected for further analysis.


### Model Diagnostics for the Refined Model Without the variable Condition


```{r 1.75}

# Plot diagnostic plots for model_refined
par(mfrow = c(2, 2))  # Set up a 2x2 grid for plots
plot(model_refined2)

```
### Final Model Equation and Interpretation

The final selected model predicts the log-transformed price of houses (\( \log(\text{price}) \)) based on 21 predictors and an intercept. 

```{r 1.76}
#Summary of selected Model
summary(model_refined2)
```
The equation is as follows:

\[
\log(\text{price}) = \beta_0 + \beta_1 \cdot \text{lat} + \beta_2 \cdot \text{sqft_living} + \beta_3 \cdot \text{yr_built} + 
\beta_4 \cdot \text{bedrooms} + \beta_5 \cdot \text{sqft_living15} + \beta_6 \cdot \text{floors} +\\
\beta_7 \cdot \text{grade4} + \beta_8 \cdot \text{grade5} + \beta_9 \cdot \text{grade6} + 
\beta_{10} \cdot \text{grade7} + \beta_{11} \cdot \text{grade8} + \beta_{12} \cdot \text{grade9} + 
\beta_{13} \cdot \text{grade10} + \\ \beta_{14} \cdot \text{grade11} + 
\beta_{15} \cdot \text{view1} + \beta_{16} \cdot \text{view2} + \beta_{17} \cdot \text{view3} + 
\beta_{18} \cdot \text{view4} + \beta_{19} \cdot \text{year_sold} + \\
\beta_{20} \cdot \text{bathrooms} + \beta_{21} \cdot \text{waterfront1} + \epsilon
\]


### Model Interpretation

- **Intercept (\( \beta_0 \)):** 

The base \( \log(\text{price}) \) when all predictors are at their reference levels, though this is not meaningful in practical terms.

- **Latitude (\( \beta_1 = 1.277 \)):**

A one-unit increase in latitude corresponds to a 1.277 increase in \( \log(\text{price}) \), holding other variables constant.

- **Square Footage (\( \beta_2 = 0.0001419 \)):** 

For every additional square foot of living space, \( \log(\text{price}) \) increases by 0.0001419.

- **Year Built (\( \beta_3 = -0.003805 \)):** 

For each additional year since construction, \( \log(\text{price}) \) decreases by 0.003805, reflecting depreciation.

- **Bedrooms (\( \beta_4 = -0.0174 \)):**

Each additional bedroom decreases \( \log(\text{price}) \) by 0.0174, likely due to reduced room sizes.

- **Other Predictors:** 

  - **Floors (\( \beta_6 = 0.05395 \))**: 
  
  Additional floors increase \( \log(\text{price}) \).
  
  - **Grade Levels (\( \beta_7 \) to \( \beta_{14} \))**: 
  
  Higher grades significantly increase \( \log(\text{price}) \), with the largest impact from grade 11.
  
  - **View Levels (\( \beta_{15} \) to \( \beta_{18} \))**:
  
  Better views positively affect \( \log(\text{price}) \), with the strongest effect from view4.
  
  - **Year Sold (\( \beta_{19} = 0.04257 \)):**
  
  Later sales increase \( \log(\text{price}) \).
  
  - **Bathrooms (\( \beta_{20} = 0.04731 \))**:
  
  Each additional bathroom increases \( \log(\text{price}) \).
  
  - **Waterfront (\( \beta_{21} = 0.1647 \)):** 
  
  Waterfront properties command a higher \( \log(\text{price}) \).



### Diagnostic Plots for Refined Model Without Condition

The diagnostic plots above evaluate the assumptions of the refined model without the `condition` predictor. 

- **Residuals vs Fitted**: This plot shows residuals scattered around zero with a slight curvature, suggesting minor non-linearity in the relationship between predictors and the response variable.

- **Q-Q Plot**: The majority of residuals align with the diagonal line, indicating approximate normality, though deviations at the tails point to potential outliers or non-normality in extreme values.

- **Scale-Location Plot**: The relatively flat red line and consistent spread of residuals suggest constant variance (homoscedasticity), with minimal heteroscedasticity.

- **Residuals vs Leverage**: This plot highlights influential points that have high leverage and may disproportionately affect the model's fit, as indicated by Cook’s distance.


### Model Selection and Recommendations

Based on the analysis, we have chosen the refined model without the `condition` predictor as our final model. This model balances explanatory power and simplicity, achieving a relatively high \( R^2 \) (0.7071) and Adjusted \( R^2 \) (0.7068) while minimizing SSE and maintaining favorable AIC and BIC scores.
Future refinements could include exploring robust regression techniques or machine learning techniques.
The chosen model provides a strong foundation for predicting house prices and delivering actionable insights.


### Reflection

This project provided valuable insights into the factors influencing house prices and the importance of rigorous model selection. A deeper understanding of the multiple regression analysis studied during the semester was enhanced through practical application. It was great to see these concepts reinforced while working on the project, bridging theoretical knowledge with real-world data analysis.







